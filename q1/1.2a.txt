The statement is true. 
Logistic regression, relying on a linear decision boundary, faces challenges in capturing the intricacies of non-linear relationships inherent in data represented by pixel values. This limitation arises from its inherent assumption of a linear relationship, which may struggle to represent the complex patterns found in images. In contrast, a multi-layer perceptron (MLP) equipped with ReLU activations demonstrates the capability to learn intricate non-linear mappings within input features, providing enhanced expressiveness in handling image data. The expressiveness of the MLP is attributed to its ability to learn hierarchical features through the combination of multiple layers, enabling it to capture complex relationships in pixel values.
The difficulty in capturing complex patterns with logistic regression stems from its linear nature, assuming a straightforward relationship between pixel values and output. This linearity constrains its ability to adapt to the intricate and non-linear patterns often present in image data. On the other hand, an MLP with ReLU activations excels in handling such complexities by virtue of its non-linear activation function, allowing it to capture and learn intricate patterns in pixel values that would be challenging for logistic regression.
Concerning the training process, logistic regression has the advantage of convex optimization, ensuring a unique global minimum during training. This simplicity arises from the convexity of the optimization problem, guaranteeing a single optimal solution that can be efficiently reached. In contrast, an MLP with ReLU activations engages in non-convex optimization, presenting challenges such as multiple local minima. This lack of convexity complicates the training process, making it more sensitive to initialization, vanishing/exploding gradients, and increasing the risk of getting stuck in suboptimal local minima.